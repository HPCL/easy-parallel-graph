\documentclass[11pt]{article}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=1in, headheight=15pt]{geometry}
\usepackage{amsmath, amsthm, amsfonts}
\usepackage{hyperref}

\usepackage{fancyhdr, setspace}
\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancy}
\setlength{\headheight}{15pt} % Removes warnings. Set to 30pt if 2 lines on the header.
\lhead{}
\rhead{}
\usepackage{totcount}
\regtotcounter{page}
\cfoot{\ifnum\totvalue{page} > 1 \thepage \else\fi}

\usepackage{enumitem}
\setlist{parsep=0pt, listparindent=0.5cm}

\usepackage{titlesec}
\titleformat*{\section}{\large\bfseries}

\begin{document}
\begin{center}
{ \huge
	Simplifying Parallel Graph Processing: \\
}
{ \Large
	Directed Research Project Contract \\
}
Sam Pollard (\href{mailto:spollard@cs.uoregon.edu}{spollard@cs.uoregon.edu}), University of Oregon \\
\today
\end{center}

\section{Introduction}
Graph processing has important differences from other High Performance Computing (HPC) applications. For example, floating point operations per second mean little when performing a breadth first search. Furthermore, properties of the input graph such as its sparsity can play a profound role in an algorithm's performance.
% bfs turns out to be memory latency-bound instead of compute or memory bandwidth-bound \cite{Beamer:2016:GAP, Eisenman:2016:Prejudice}.
Because of these differences, there has been movement toward standardizing performance measurements specifically directed at graph processing. The largest example is the Graph500 \cite{Murphy:2010:Graph500}, a ranking of supercomputers in the spirit of the Top500. Other projects such as Graphalytics \cite{Guo:2014:Graphalytics} attempt to compare performance across various platforms while projects such as the Graph Algorithm Platform Benchmark Suite \cite{Beamer:2016:GAP} and GraphBIG \cite{Nai:2015:Graphbig} attempt to provide high quality, reference implementations next to which performance can be compared. Here, we use the definition of \emph{platform} defined in \cite{Guo:2014:Graphalytics}: ``the combined hardware, software, and programming system that is being used to complete a graph processing task.''

There are a plethora of programming paradigms, domain specific languages, and libraries for parallel graph processing. Together these cover a diverse range of applications, programming languages, and target architectures. Whereas the Graph500 measures scalability, there is impressive research on achieving the highest performance per core on shared memory systems. For example, the GraphChi platform in particular showed performance on a personal computer can have performance comparable to a medium-sized cluster \cite{kyrola:2012:Graphchi}.

Publications introducing new platforms generally compare their performance to popular existing platforms on a few standard datasets. However, these performance measurements are naturally biased towards the authors' works. Thus, when attempting to solve a graph-based problem, the question of \emph{which} platform to use is daunting. Beyond this, platforms have unique input file formats, programming paradigms, configuration demands, and dependencies, all of which combine to yield a steep learning curve and a lack of portability, which are concerns applicable to HPC in general.

\section{Project Description}
This project aims to analyze the performance of graph algorithms and recommends an optimal platform to the user based on this analysis. The project aims to look beyond the number of traversed edges per second (TEPS) on just breadth-first search as in the Graph500. Potential measurements include strong and weak scalability, load balancing, memory utilization, and disk usage. The Tuning and Analysis Utilities (TAU) will be investigated as a potential source for these measurements \cite{Shende:2006:Tau}. Beyond performance analysis, hardware and middleware specifications will be reported such as data transport mechanisms and scheduling paradigms as inspired by \cite{Firoz:2016:Reportcard}.

% Idea: Measure scalability for different problem sizes differently: measure 2 classes: one in which distributed systems make the most sense and one for which shared memory systems make the most sense (and find that magic number!). Define \emph{scale} as $\log_10(|E| + |V|)$ \cite{Iosup:2016:Graphalyticstech}.

% Also: May want to list all the existing graph processing benchmarks: GAP, Graphalytics, GraphBIG, BigBench 2.0 (in development).

%The goal of such detailed measurement is to make the selection of a particular graph processing platform easier; these data can be used to moref intelligently direct programmer effort. This project aims to create a utility to make these measurements, but also takes in user-specified architecture and graph properties to intelligently select the optimal platform from a performance standpoint.

This project will begin by building on top of the graph benchmarking tool Graphalytics (described in \cite{Guo:2014:Graphalytics}) with increased focus on describing the hardware, communication paradigms, scheduling, and other lower-level details as recommended by \cite{Firoz:2016:Reportcard}. Preliminary research has shown Graphalytics to be unportable and challenging to extend, so this project will provide a much-needed simplification over existing frameworks. Furthermore, this project will standardize performance analysis for each platform. This will be accomplished through comprehensive analysis of all prominent platforms. Additionally, modeling of the target architectures will be used to explain the observations performance and to predict performance on new architectures.

Lastly, this project aims to use these data to provide recommendations so users can make informed decisions about which platform is optimal for a given architecture, problem size, and algorithm. At their simplest, these recommendations will be based on existing measurements and of the form: ``given machine $X$ running algorithm $Y$ on a problem of size $Z$, the optimal platform is $P$.'' This format is restrictive (the user may not know $X$, $Y$, or $Z$) and does not address the cost of programming on a new platform for each problem. The first step to address this would be to use a model of graph processing performance to predict the optimal platform with less information. Time permitting, the use of a domain specific language to translate high level algorithm specifications to the optimal platform will be investigated.
% To foster increased developer interaction, an online leaderboard could be created which ranks platforms by various metrics such as scalability (for small problem sizes), scalability (for large problem sizes), power efficiency, simplicity of implementation (number of steps required to get the problem solved), and performance for various algorithms. This would be done to spark competition and to motivate developers to provide high quality implementations of each algorithm.

\section{Timeline}
	\begin{enumerate}
		\item[Fall '16.] Continue literature search about benchmarking for graph processing algorithms. Decide on two or three platforms as starting points to analyze performance and tabulate basic performance measurements. Look into methods to increase portability such as Spack \cite{Gamblin:2015:Spack}.
		% \item Apply for the NDSEG fellowship by \textbf{December 9}.
		\item[Winter.]  Get preliminary measurements and begin to analyze which platforms are optimal for a given input, algorithm, and hardware. Create a model to explain observed performance differences. Use these data to make simple platform recommendations. Coalesce all noteworthy graph processing platforms, prepare an exhaustive list of these platforms. This may be the beginnings of an area exam survey paper. Time and resource-permitting: install a power monitor on the research hardware to increase quality and depth of performance measurement.
		% 		\item Select the most prominent and promising of these platforms for detailed analysis.
		\item[Spring.] Keep up with the state of the art by adding more platforms and architectures to the measurements. Enhance portability by testing on various architectures. Compile performance results and begin writing the paper.
		\item[Summer.] Enhance model of platform, algorithm, and dataset performance. Improve recommender to work in cases where the user does not have complete knowledge of the problem to be solved. Write paper and investigate which conference or journal is appropriate.
		\item[Fall '17.] Submit the paper to a conference or journal. Present to DRP committee. Time permitting: investigate viability of a domain specific language (DSL). Develop the DSL so it can express the most popular algorithms on the most popular platforms.
		% \item (Time Permitting) Make an online leaderboard which measures a platform, not a supercomputer: this would rank graph processing frameworks by various aspects such as scalability and computational efficiency.
		% \item (Time Permitting) Make an interative tool where you select some number of choices (always including "I don't know" as a possible choice), and the tool spits out which platform to use.
	\end{enumerate}

\section{Deliverables}
	\begin{enumerate}
		\item A comprehensive list of graph processing platforms and descriptions of them. Categorize these by prominence so analysis and porting effort is well-spent.
		\item Machine and human readable, complete, precise performance analysis for several graph processing platforms on the existing research hardware\footnote{This machine is named Arya and is an Intel Xeon-based machine with 72 cores, an NVIDIA GTX 980 GPU and 256GB RAM} and the UO cluster. These measurements will be performed on several standardized datasets and the most common algorithms such as breadth first search, local clustering coefficients, single-source shortest paths, PageRank, and community detection.
		\item A recommender tool which can be easily run on various architectures.
		\item A paper to submit to a conference or journal.
		\item A workshop-style tutorial of this recommender tool.
		\item Time permitting: a proof of concept DSL.
	\end{enumerate}

\section{Committee Members}
\begin{enumerate}
	\item Boyana Norris
	\item Allen Malony
	\item Zena Ariola
\end{enumerate}
\bibliographystyle{acm}
\bibliography{drp}
\end{document}