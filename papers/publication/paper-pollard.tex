\documentclass[conference]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{cite} % Multiple citations in one \cite command
\usepackage{pgfplotstable}
\usepackage{array} % Required for pgfplotstable dec sep align
\pgfplotsset{compat=1.12}
\usepackage{booktabs}
\usepackage{todonotes}

\usepackage{enumitem}
\setlist{parsep=0pt, listparindent=0.5cm}

\begin{document}
\title{A Comparison of Parallel Graph Processing Implementations}
\author{
	\IEEEauthorblockN{Samuel D. Pollard}
	\IEEEauthorblockA{Computer and Information Science\\
		University of Oregon \\
		Eugene, OR 97403\\
		Email: spollard@cs.uoregon.edu}
	\and
	\IEEEauthorblockN{Boyana Norris}
	\IEEEauthorblockA{Computer and Information Science\\
		University of Oregon \\
		Eugene, OR 97403\\
		Email: norris@cs.uoregon.edu}
}
\maketitle
\begin{abstract}
The rapidly growing number of large network analysis problems has led to the emergence of many parallel and distributed graph processing systems---one survey in 2014 identified over 80. Since then, the landscape has evolved; some packages have become inactive while more are being developed. Determining the best approach for a given problem is infeasible for most developers. To enable easy, rigorous, and repeatable comparison of the capabilities of such systems, we present an approach and associated software for analyzing the performance and scalability of parallel, open-source graph libraries. We demonstrate our approach on five graph processing packages: GraphMat, the Graph500, the Graph Algorithm Platform Benchmark Suite, GraphBIG, and PowerGraph using synthetic and real-world\todo{actual real-world. (i) Michael LeBeane et al, Supercomputing (SC 15), Data Partitioning Strategies for Graph Workloads on Heterogeneous Clusters (ii) Shuang Song et al,  ICPP 2016, Proxy-Guided Load Balancing of Graph Processing Workloads on Heterogeneous Clusters.} datasets. We examine previously overlooked aspects of parallel graph processing performance such as phases of execution and energy usage for three algorithms\todo{more algorithms, justify why these algorithms}: breadth first search, single source shortest paths, and PageRank and compare our results to Graphalytics.
\end{abstract}

\section{Introduction}
%% What's the problem (motivation)
Our research is motivated by the current state of parallel graph processing. The most comprehensive survey, released in 2014, identified and categorized over 80 different parallel graph processing systems, not even considering domain specific languages \cite{Doekemeijer:2015:GPFSurvey}.

An overarching issue among these systems is the lack of comprehensive comparisons. One possible reason is the considerable effort involved in getting each system to run: satisfying dependencies and ensuring data are correctly formatted can be time consuming tasks. Additionally, some systems are developed with large, multi-node clusters in mind while others only work with shared memory, single node computers. An example of the former is GraphX \cite{Xin:2013:GraphX}, which incurs some overhead while gaining fault tolerance and an example of the latter is Ligra \cite{Shun:2013:Ligra}, a framework requiring a shared-memory architecture.
% ``The project was motivated by the fact that the largest publicly available real-world graphs all fit in shared memory''

From a graph algorithm user's point of view, optimizations for each system may not be apparent. For example, the Parallel Boost Graph Library (PBGL)~\cite{Gregor:2005:PBGL} provides generic implementations of their algorithms and the programmer must provide the template specializations. Optimal data structures may differ across graphs and must be determined by the programmer. To complicate matters further, the $\Delta$-stepping implementation of the parallel Dijkstra's Algorithm for computing single source shortest paths is parametrized by some $\Delta > 0$ for which requires tuning for a given architecture and graph structure to achieve optimal performance \cite{Panitanarak:2014:SSSPPerf}. \todo{Will this be mentioned again?}

We aim to simplify the decision making process by providing results from our experiments on both real world and synthetic datasets. We take inspiration from the Graph500 benchmark \cite{Murphy:2010:Graph500} which clearly specifies every step of the breadth first search (BFS) algorithm and how it should be timed. The Graph500 can be used to fairly rank systems on a single, well-defined benchmark. In this context, \emph{system} is defined as the hardware, operating system, middleware, and algorithmic implementation which accomplishes a certain task, in this case BFS.

While leaderboards can indicate computational milestones, they are not particularly useful for the average user. Reference implementations, however, are critical for advancement; the complexity of today's hardware and operating systems require empirical evidence to assess an algorithm's performance. Additionally, there is motivation to define basic building blocks for graph computations \cite{GABB16, Buluc:CombBLAS:2011}. However, new algorithm designers and users alike have no easy way of determining what constitutes a high-performance implementation.

To address these issues, we present \emph{easy-parallel-graph\mbox{-\textasteriskcentered}}
% Here, \textasteriskcentered\ could be \emph{processing}, \emph{performance-analysis}, or \emph{algorithm-comparison})
\footnote{Available at \url{https://github.com/HPCL/easy-parallel-graph}}, a utility which simplifies the installation, comparison, and performance analysis of the three most widely implemented graph algorithm building blocks: breadth first search (BFS), single source shortest paths (SSSP), and PageRank (PR). With this tool we select a small number of graph processing libraries over which we homogenize all aspects of execution to fairly compare performance. Additionally, we present an example of this framework via analysis of the performance, energy consumption, and scalability results to enable algorithm designers to select high performance reference implementations with which they can compare their improvements. To be clear, we are not proposing a new benchmark suite or providing any reference implementations. We provide a framework to compare existing software packages without requiring the user to be familiar with them.

\section{Related Work}
\todo{There's a large number of qualitative. Most third party comparisons are qualitative and just describe the overview instead of actual runtimes. This isn't a level playing field.}
Most performance analysis of graph processing systems come from the empirical results of each new library designer's publications. For example, GraphMat \cite{Sundaram:2015:GraphMat}, PowerGraph \cite{Gonzalez:2012:Powergraph}, and GraphBIG \cite{Nai:2015:Graphbig} present a comparison of performance between their system and a selection of other software packages.

Nai et al.~\cite{Nai:2016:architectural} provide a detailed performance analysis using GraphBIG as their reference implementation. Their approach also compares GraphBIG to the GraphLab and Pregel execution models. Their analysis considers architectural performance measurements such as cache miss rates to measure bottlenecks and computational efficiency for a variety of datasets. While incredibly thorough, these analyses focus on covering a wide range of datasets and appear difficult to apply to a new approach. For example, GraphMat reduce computation to sparse matrix operations which may not suffer from the same memory bottlenecks indicated in the results of Nai et al.

Beyond the aforementioned performance results there are a number of existing reports on performance analysis not tied to a particular software package we presently describe.

Satish et al.~\cite{Satish:2014:NavigatingGraph} analyze the performance of several systems on datasets on the order of $30$ billion edges and \cite{Lu:2014:ExperimentalEval} uses six real-world datasets and focuses on the vertex-centric programming model. These implementations are hand tuned and provides recommendations for future improvements.

The most prominent example of a graph processing performance analysis tool not tied to a particular iplementation is Graphalytics \cite{Capota:2015:Graphalytics}. Like our work, Graphalytics also automates the setup and execution of graph packages for performance analysis. Graphalyics relies on Apache Maven and Java to wrap each graph processing software package---Graphalytics calls these platforms---and requires configuration files for each dataset.\todo{What am I saying with this?}

Moreover, the complexity of Graphalytics can obfuscate the true behavior of the program. By default, Graphalytics generates an HTML report listing the runtimes for each dataset and each algorithm in seconds. For example, one run of a single source shortest paths algorithm took 5.96 seconds. However, perusing the log files reveals a more complete picture: of these 5.96 seconds, 0.21 was spent computing the shortest paths while the remaining time consisted of building the necessary data structures. Furthermore, other performance-relevant details, such as the number of iterations run for PageRank, are not easily available using Graphalytics.
%We use GraphMat, the single source shortest paths algorithm, the dota-league dataset from the Game Trace Archive\cite{Guo:2012:GTA}, and the default settings as an example here.
% The full logs:
%Finished setting ids, time: 0.157009
%Starting sort
%Finished sort, time: 1.39571
%Finished setting edge pointers, time: 0.000133
%Starting build_dcsc
%Finished build_dcsc, time: 0.235948
%Finished setting ids, time: 0.154693
%Starting sort
%Finished sort, time: 1.75676
%Finished setting edge pointers, time: 0.000142
%Starting build_dcsc
%Finished build_dcsc, time: 0.238386
%Completed reading A from memory in 4.089144 seconds.
%Completed reading A from file in 5.114708 seconds
%Completed 19 iterations
%Timing results:
%- load graph: 5.11472 sec
%- initialize engine: 2.88486e-05 sec
%- run algorithm: 0.212451 sec
%- print output: 0.0799532 sec
%- deinitialize engine: 2.88486e-05 sec
%17:29:05.575 [INFO ] Benchmarking algorithm "Single source shortest paths" on graph "dota-league" took 5955 ms.

With a plugin to Graphalytics called Granula \cite{Ngai:2015:Granula}, one can explicitly specify a performance model to analyze specific execution behavior such as the amount of communication or runtime of particular kernels of execution. This requires in-depth knowledge of the source code and execution model. Furthermore, creating such a model requires a high level of expertise with the given system and with Granula\footnote{An example of Granula can be seen at \url{https://github.com/tudelft-atlarge/graphalytics-platforms-graphx/tree/master/granula-model-graphx}}.

As with Graphalytics, the initial development effort is high but Granula paired with Graphalytics allows automatic execution and compilation of performance results. Likewise, our approach provides automatic execution and performance analysis but without requiring a performance and execution model for each system.

Beyond this plethora of performance data, the Graph Algorithm Platform Benchmark Suite (GAP) \cite{Beamer:2015:GAPBench}, GraphBIG, and the Graph500 \cite{Murphy:2010:Graph500} consider themselves reference implementations or benchmark suites next to which other implementations can be compared. If no fewer than three software packages call themselves, ``reference implementations,'' which one are we to trust? Our belief is choosing a single reference implementation cannot capture the complexities inherent with graph processing.

Our contribution strikes a balance between the highly detailed analysis of a single implementation such as those presented by Nai \cite{Nai:2016:architectural} (greater depth of analysis) and Graphalytics which focuses on the runtimes of several implementations and several platforms (more breadth of analysis). This allows us to greatly simplify deployment and allows the user to make his or her own decisions.

In addition, we analyze  power and energy consumption as well as details such as the number of iterations or the time to construct the data structures. Moreover, our approach requires little knowledge of the inner workings of each system; the data are collected using either hardware counter sampling of model-specific registers (for power) or through parsing log files (for execution time). This allows arbitrary datasets to be included as long as they are in the same, simple format as those in the Stanford Network Analysis Project (SNAP) datasets \cite{snapnets}.

% The systems described in \cite{Doekemeijer:2015:GPFSurvey} operate with a wide range of parallelism paradigms and target architectures such as GPU \cite{Zhong:2014:Medusa, Kang:2009:Pegasus}, shared memory CPU \cite{Shun:2013:Ligra, kyrola:2012:Graphchi, Nguyen:2013:Galois}, a combination of CPU and GPU \cite{Gharaibeh:2012:Totem}, distributed filesystem based approaches \cite{Xin:2013:GraphX}, and distributed memory with MPI \cite{Gregor:2005:PBGL}.

% Beyond the systems described by Doekemeijer and Varbanescu, the problem has compounded with the addition of even more proprietary and open source projects such as \cite{Cheramangalath:2015:Falcon, Perez:2015:Ringo}, distributed memory approaches such as \cite{Hong:2015:PGX}. domain-specific languages \cite{Hong:2012:GreenMarl}, distributed database querying, \cite{Rodriguez:2015:Gremlin}, as well as novel communication schemes \cite{Edmonds:2013:ActiveMessages}. At the outset, this plethora of choices makes the question, ``which system is the best for my problem?'' daunting.

% In addition to libraries with associated APIs there has also been a propagation of ``reference implementations'' which implement the most common graph algorithms such as \cite{Beamer:2015:GAPBench, Nai:2015:Graphbig}. Thus, even selecting a standard and a benchmark over which to compare various implementations is nontrivial. To quote Andrew Tanenbaum, ``The nice thing about standards is that you have so many to choose from.''

\section{Methodology}
\todo{Start with dataset subsection, write new intro to methodology, move this into inputs. We're comparing algorithms not applications. Understand those since they're the building blocks. Not comparing architectural details (cite that Nai paper). Important paragraph.}
Our utility is broken down into five principal phases indicated in \figurename~\ref{fig:epg-overview}, each of which requires no more than a single shell command. The five phases are as follows:
\begin{enumerate}
	\item Installing modified, stable forks of each software package to ensure homogeneity.
	\item Given a synthetic problem size or a real-world graph file, generate the files necessary to run each software package.
	\item Given a graph and the number of threads, run each algorithm using each software package multiple times.
	\item Parse through the log files to compress the output into a CSV.
	\item Analyze the data using the provided R scripts to generate plots.
\end{enumerate}

\begin{figure*}
	\centering
		% trim=left lower right upper
		\includegraphics[width=\linewidth, trim=0 144pt 0pt 156pt, clip]{graphics/overview.eps}
	\caption{Overview of \emph{easy-parallel-graph-\textasteriskcentered}. Each orange box corresponds to a single shell script in the package while the green ellipses correspond to generated files.}
	\label{fig:epg-overview}
\end{figure*}

\subsection{Installing Libraries}
The libraries are stable forks of the given repositories which are configured to ensure the experiments execute in the same manner.

%% How do we address these issues?
Our experiments use the author-provided implementations with modifications only to insert performance analysis hooks or to ensure homogeneous stopping criteria; we assume the developers of each system will provide the best performing implementation. While this limits the scope of the experiments it mitigates the bias inherent in our programming skills in addition to the bias of library designers' performance analysis; a given library designer will understand his or her source code better than any other implementation.

\subsection{Datasets}
Some software packages provide serialized file formats to speed up file IO. Homogenizing the datasets creates extra copies of the graph files.

We refer to a graph's \emph{scale} when describing the size of the graph. Specifically, a graph with scale $S$ has $2^S$ vertices. For example, many of our experiments were run with $2^{22} = 4,194,304$ vertices and an average of 16 edges per vertex. We measure strong scaling and speedup by varying the number of threads from one to the total number of threads on our server, 72.

Each experiment uses 32 roots per graph. As with the Graph500, each root is selected to have a degree greater than 1. For PageRank, we simply run the algorithm 32 times. We plot many of the results as box plots with an implied 32 data points per box.

When possible, we measure the dataset construction time as the time to translate from the unstructured file data in RAM to the graph representation on which the algorithm can be performed. This is not possible for PowerGraph and GraphBIG because they read in the input file and build a graph simultaneously.

We use the Graph500 synthetic graph generator which creates a Kronecker graph \cite{Leskovec:2010:Kronecker} with initial parameters of $A = 0.57, B = 0.19, C = 0.19,$ and $D = 1-(A+B+C) = 0.05$ and set the average degree of a vertex as 16. Hence, a Kronecker graph with scale $S$ has $2^S$ vertices and approximately $16 \times 2^S$ edges.

\todo{More real world datasets}
The Graphalytics results in Table~\ref{tab:graphalytics} were performed on the Dota-League datasets. This dataset contains 61,670 vertices and 50,870,313 edges. This dataset comes from the Game Trace Archive\cite{Guo:2012:GTA} and is modified for Graphalytics\footnote{This dataset is available at \url{https://atlarge.ewi.tudelft.nl/graphalytics/}.}. This dataset is useful because it is both weighted and more dense than the usual real-world dataset with an average out-degree of 824.

\subsection{Graph Processing Systems}

This study explores four shared memory parallel graph processing platforms and one distributed memory framework. The first three are so-called ``reference implementations'' while the remaining two are included because of their performance and popularity. Other popular libraries such as the Parallel Boost Graph Library \cite{Gregor:2005:PBGL} are not considered here because the authors do not provide reference implementations. We consider the following systems.
\begin{itemize}
	\item The Graph500\footnote{We used the most recent version from \url{https://github.com/graph500/graph500}, most similar to release 2.1.4.} \cite{Murphy:2010:Graph500}, a set of large-scale graph benchmarks.
	\item The Graph Algorithm Platform (GAP) Benchmark Suite \cite{Beamer:2015:GAPBench}.
	\item GraphBIG \cite{Nai:2015:Graphbig} benchmark suite.
	\item GraphMat \cite{Sundaram:2015:GraphMat}, a high-performance graph library.
	\item PowerGraph \cite{Gonzalez:2012:Powergraph}, a framework for distributed graph-parallel computation on natural graphs.
	\todo{Explain each more.}
\end{itemize}

Our approach is not specific or limited to these graph packages and can be extended to others with relatively modest effort (building each package and if necessary, instrumenting the different phases of the computation).
\todo{Say what we automate and what we don't automate.}
\todo{Weighted versus unweighted}

\subsection{Algorithms}\label{sec:algs}

We consider three algorithms: Breadth First Search (BFS), Single Source Shortest Paths (SSSP), and PageRank, although not all algorithms are implemented on all systems. We inspected the source code of the surveyed parallel graph processing systems to ensure the same phases of execution are measured across differing execution and programming paradigms. 

We select BFS because the canonical performance leaderboard for parallel graph processing is the Graph500 \cite{Murphy:2010:Graph500}. The advantage of the Graph500 is that it provides standardized measurement specifications and dataset generation. The primary drawback with the Graph500 is it measures a single algorithm.

Our work aims to add similar rigor to other graph algorithms by borrowing heavily from the Graph500 specification. The Graph500 Benchmark 1 (``Search'') is concerned with two kernels: the creation of a graph data structure from an unsorted edge list stored in RAM and the actual BFS\footnote{For a complete specification, see \url{http://graph500.org/specifications}}. We run the BFS using 32 random roots with the exception of PowerGraph which doesn't provide an reference implementation of BFS in its toolkits.

We select SSSP because of the straightforward extension from BFS; we need not modify the graph nor the root vertices from BFS.

PageRank is selected because of its popularity; most libraries provide reference implementations. One challenge with using PageRank is the stopping criterion; all implementations have been modified to use $|| p_t - p_{t-1}||_1$ (the absolute sum of differences) where $p_t$ is the pagerank at step $t$. Verification of the PageRank results is beyond the scope of this paper, although this may explain some of the large performance discrepancies.\todo{Better explain why these were selected. They're the most popular of the algorithms for the papers we surveyed.}

This approach is not specific to a particular algorithm; measuring the execution time, data structure construction time, and power consumption can be applied easily to other algorithms.

\subsection{Parsing and Data Analysis}
Our example analyzes data using the R programming language and shell scripting parsers. The example workflow provided in our utility yields the figures generated in this paper.

\subsection{Machine Specifications}
We performed experiments on our 36-core (72 thread) Intel Haswell server with 256GB DDR4 RAM, with two Intel Xeon E5-2699 v3 CPUs. The operating system is GNU/Linux version 4.4.0-22.
%The disparity between the CPU's advertised clock speed and the ``CPU Clock'' row is a result of the Turbo Boost technology which can increase the clock speed to a limit. We use the manufacturer's published maximum clock speeds which can be found at \url{http://ark.intel.com}.
%\begin{center}
%	% For arya I deleted Max RAM Freq	2133MHz
%	\pgfplotstabletypeset[
%	header=false,
%	col sep=tab,
%	string type,
%	every head row/.style={output empty row, before row=\bottomrule},
%	columns/0/.style={column type={|p{1in}|}},
%	columns/1/.style={column type={p{3.2in}|}},
%	every last row/.style={after row=\toprule},
%	]{../report/specs.csv}
%	%\label{tab:specs}
%\end{center}

\section{Performance Analysis}\label{sec:perf}

We analyze the performance of the algorithms described in Sec.~\ref{sec:algs} in terms of execution time, scalability over multiple threads, and power and energy consumption\todo{ A precise list and exposition of the requirements and changes	done to the application would help in understanding the methodology and its implications}. All figures except those measuring scalability (Fig.~\ref{fig:bfs-scaling}) use 32 threads.

\subsection{Runtime}

In Table~\ref{tab:graphalytics} we show the results from running Graphalytics on a Kronecker graph of scale $22$ (4,194,304 vertices and approximately $16 \times 2^22 = 33,500,000$ edges)\todo{work?}. An explanation of each algorithm is given in \cite{Iosup:2016:Graphalyticstech}. Graphalytics by default does not perform SSSP on unweighted, undirected graphs. The discrepancy between PageRank values in Table~\ref{tab:graphalytics} and Fig.~\ref{fig:pr} is a result of the differing stopping criterion.\todo{What are the likely culprits for performance differences---measure cache miss rates. Try to find something which correlates with performance. Resource stalls will work easily (not as interesting) Cycles per instruction? Automate with perfexplorer or autoperf}

In addition to the data presented in Table~\ref{tab:graphalytics}, one may want to know the cost of performing additional computations on a graph once the data structures have been built. Additionally, one may desire runtime bounds to get a reasonable maximum timespan for a given problem. Table~\ref{tab:epg-perf} begins to compile performance results and the rest of this section further explores these topics.

% Graphalytics also outputs MTEPS or millions of traversed edges per second. However, the graphalytics version does not make sense in all cases: for example, computing the local clustering coefficient involves traversing each edge multiple times (proportional to the sparsity of the graph), while BFS traverses each edge exactly once, and the number of edges traversed with PageRank depends on the connectivity of the graph and the number of iterations.

\begin{table}
	\caption{The dataset used is the same Kronecker graph with scale $22$ as used in other experiments. Performance results are in seconds running with the maximal number of threads (64 for Powergraph, 72 for the others). Community detection is performed using label propagation.}
	\centering
	\pgfplotstabletypeset[
		col sep=comma,
		columns={[index]0,graphmat,openg,powergraph},
		every head row/.style={after row=\midrule},
		columns/0/.style={string type, column type={l|}, column name={}},
		columns/graphmat/.style={
			column name={GraphMat},
			string replace={0}{},
			dec sep align,
			empty cells with={N/A}
		},
		columns/openg/.style={
			column name={GraphBIG},
			dec sep align},
		columns/powergraph/.style={
			column name={PowerGraph},
			dec sep align}
	]{../report/runtime.csv}
	\label{tab:graphalytics}
\end{table}


\begin{table}
	\centering
	\caption{This table shows averages of runtimes for the Dota-League dataset using our method using 32 threads. Each cell represents the average over 32 runs. The ``N/A'' for PowerGraph and GraphBIG exist because both of these systems read in the files and build a graph data structure simultaneously. GraphMat is not included because reading in the Dota-League dataset crashes the system.}
	\begin{tabular}{l|c|c|c|c}
		System & \begin{tabular}[x]{@{}c@{}}Construct Data \\ Structure\end{tabular}
		                   & Run BFS & Run SSSP & Run PageRank \\ \hline
		PowerGraph & N/A   & N/A     & 0.344    & 3.606 \\
		GAP        & 0.412 & 0.00464 & 0.267    & 33.00 \\
%		GraphMat   & 1.013 &         &          &       \\
		GraphBIG   & N/A   & 0.0476  & 0.162    & 21.39 \\
	\end{tabular}
	\label{tab:epg-perf}
\end{table}

Figures~\ref{fig:bfs-time} and \ref{fig:sssp-time} show performance results for a Kronecker graph with scale $22$. The box plots give an idea of the runtime distributions. There is less variance in the runtimes of SSSP (between 0.1 and 1.7 seconds) compared to BFS (0.01 and 1.7 seconds) but GAP is the clear winner in both cases. The data structure construction times for GAP and GraphMat are consistent; in both cases the platforms create the same data structure for both algorithms. These results are consistent with \cite{Sundaram:2015:GraphMat} which lists GraphMat as more performant than PowerGraph in SSSP.

\begin{figure}
	\centering
	\begin{minipage}{0.48\linewidth}
		\includegraphics[width=\linewidth, trim=0 36pt 18pt 0, clip]{graphics/bfs_time.pdf}
	\end{minipage}
	\begin{minipage}{0.48\linewidth}
		\includegraphics[width=\linewidth, trim=0 36pt 18pt 0, clip]{graphics/bfs_dsc.pdf}
	\end{minipage}
	\caption{The $y$-axes are logarithmic. The left box plot shows the time to compute BFS on 32 random roots while the right plot shows the times to construct the graph for each system. The Graph500 only constructs its graph once. GraphBIG reads in the file and generates the data structure simultaneously so is omitted.}
	\label{fig:bfs-time}
\end{figure}

\begin{figure}
	\centering
	\begin{minipage}{0.59\linewidth}
		\includegraphics[width=\linewidth, trim=0 36pt 18pt 0, clip]{graphics/sssp_time.pdf}
	\end{minipage}
	\begin{minipage}{0.365\linewidth}
		\includegraphics[width=\linewidth, trim=0 36pt 18pt 0, clip]{graphics/sssp_dsc.pdf}
	\end{minipage}
	\caption{The $y$-axes is logarithmic. The left box plot shows the time to compute the SSSP starting at the same 32 roots as Fig.~\ref{fig:bfs-time}. Both PowerGraph and GraphBIG construct their data structures at the same time as they read the file.}
	\label{fig:sssp-time}
\end{figure}

The behavior of PageRank is slightly different. As with SSSP and BFS, the GAP Benchmark Suite is the fastest but it also requires the fewest iterations. We attempt to define similar stopping criteria for each system, but GraphMat executes until no vertices change rank; effectively its stopping criterion requires the $\infty$-norm be less than machine epsilon. This could account for the increased number of iterations. We adjusted the other systems to use $\sum_{k=1}^{n} |p_k^{(i)} - p_k^{(i-1)}| < \epsilon $ as the stopping criterion, where $i$ is the iteration and $n$ is the number of vertices. We use $\epsilon = 6 \times 10^{-8}$ because this value is approximately machine epsilon for a single precision floating-point number. The goal is to make the stopping criteria for all implementations as similar as possible.

Moreover, the logarithmic scale in Figures~\ref{fig:bfs-time} and~\ref{fig:pr} compresses the apparent variance in runtime. Each platform in Fig.~\ref{fig:pr} has a relative standard deviation between $1/4$ and $1/2$ that of the same system executing SSSP.

\todo{The y-axis is mislabeled. Should be "iterations" for pagerank.}
\begin{figure}
	\centering
	\begin{minipage}{0.48\linewidth}
		\includegraphics[width=\linewidth, trim=0pt 18pt 18pt 0pt, clip]{graphics/pr_time.pdf}
	\end{minipage}
	\begin{minipage}{0.48\linewidth}
		\includegraphics[width=\linewidth, trim=0 18pt 18pt 0pt, clip]{graphics/pr_iters.pdf}
	\end{minipage}
	\caption{The $y$-axis is logarithmic only for the left figure. GraphMat continues to run until none of the vertices' ranks change. For the others, we use the stopping condition that the sum of the changes in the weights is no more than $6 \times 10^{-8}$, or approximately machine epsilon for single precision floating point numbers.}
	\label{fig:pr}
\end{figure}

The difficulty in comparing iteration counts for PageRank underscores an important challenge for any comparison of graph processing systems. The assumptions under which the various platforms operate can have a dramatic effect on the program. For example, the GAP Benchmark Suite stores vertex weights as 32-bit integers. However, other systems store them as floating-point numbers. This may affect performance in addition to runtime behavior in cases where weights like $0.2$ are cast to $0$. Similarly, how a graph is represented in the system (e.g., weighed or directed) may have performance and algorithmic implications but is not always readily apparent.

Figure~\ref{fig:bfs-scaling} shows the scalability and speedup for BFS. We define the strong scaling as $T_1 / (n T_n)$ where $T_1$ is the serial time, $n$ is the number of threads, and $T_n$ is the time for $n$ threads. A linear scalability is when $T_n = T_1/n$ and is the horizontal line on the bottom plot in Fig.~\ref{fig:bfs-scaling}. These plots show in general poor scaling throughout, a challenge facing most parallel graph algorithms. Additionally, $2^{20}$ vertices is a ``small'' graph by today's standards and thus library designers focus on scalability for larger graphs. Another limitation may be the ability for OpenMP to efficiently handle such a large number of threads per machine.
\begin{figure}
	\centering
	\begin{minipage}{\linewidth}
		% trim=left lower right upper
		\includegraphics[width=\linewidth, trim=0 18pt 18pt 12pt, clip]{graphics/bfs_speedup.pdf}
	\end{minipage}
	\begin{minipage}{\linewidth}
		\includegraphics[width=\linewidth, trim=0 18pt 18pt 12pt, clip]{graphics/bfs_ss.pdf}
	\end{minipage}
	\caption{The top figure has both axes logarithmic with an exception at 72 threads for readability. Linear speedup is. GraphBIG dips below 1 because it is slower for $2$ threads than for 1. $T_1$ is the serial time, $n$ is the number of threads, and $T_n$ time for $n$ threads.}
	\label{fig:bfs-scaling}
\end{figure}

\subsection{Power and Energy Consumption}
We use the Performance Application Programming Interface (PAPI) \cite{Browne:2000:PAPI} to gain access to Intel's Running Average Power Limit (RAPL), which provides a set of hardware counters for measuring energy usage. We use PAPI to obtain average energy in nanojoules for a given time interval. We modify the source code for each project to time only the actual BFS computation and give a summary of the results in Table~\ref{tab:power}. In our case, the fastest code is also the most energy efficient, although with this level of granularity we could detect circumstances where one could make a tradeoff between energy and runtime.

\begin{table}
	\caption{The data are generated using a Kronecker graph with a scale of 22 and using 32 threads. Sleeping Energy refers to the power (in Watts) consumed during the \texttt{unistd} C \texttt{sleep} function, multiplied by the Time row. Essentially, this measures the amount of energy that would have been consumed if nothing was running. The increase over sleep is the ratio of the first and third columns. These are all averaged over 32 roots.}
	\centering
	\begin{tabular}{l|r|r|r|r}
			&	GAP  &    Graph500 & GraphBIG & GraphMat \\ \hline
		Time (s) &  0.01636 & 0.01884 & 1.600 & 1.424 \\
		Average Power per Root (W) & 72.38 & 97.17 & 78.01 & 70.12 \\
		Energy per Root (J) &	1.184 & 1.830 & 112.213 & 111.104 \\
		Sleeping Energy (J) & 0.4046  & 0.4660 & 39.591 &  35.234 \\
		Increase over Sleep & 2.926 & 3.928 & 2.834 & 3.153
	\end{tabular}
	\label{tab:power}
\end{table}

RAPL also allows the measurement of DRAM power, the results of which are shown in Fig.~\ref{fig:power}. The right plot describes the second row of Table~\ref{tab:power} in more detail. We notice a smaller spread of RAM power consumption, but still a noticeable difference. GraphMat exhibits the lowest average RAM power consumption (but not the shortest execution time). This information can be useful when choosing an algorithm to use in limited power scenarios, where a slower algorithms that will not exceed the power cap is preferred to a faster one that may exceed it.

\begin{figure}
	\centering
	\begin{minipage}{0.48\linewidth}
		\includegraphics[width=\linewidth, trim=0 36pt 18pt 0, clip]{graphics/bfs_ram_power.pdf}
	\end{minipage}
	\begin{minipage}{0.48\linewidth}
		\includegraphics[width=\linewidth, trim=0 36pt 18pt 0, clip]{graphics/bfs_cpu_power.pdf}
	\end{minipage}
	\caption{Similar to Fig.~\ref{fig:bfs-time} we plot RAM and CPU Power Consumption for each of the 32 roots. Since the Graph500 runs multiple roots per execution, we only get a single data point. The baseline was computed by monitoring power consumption while a program containing only one call to the C \texttt{unistd} library function \texttt{sleep(10)} (ten seconds).}
	\label{fig:power}
\end{figure}

\section{Future Work and Conclusion}
\todo{Maybe; it's still difficult to add more graphs to the dataset sinc you need to know all the metadata.}
We have presented a systematic approach to analyzing parallel graph processing performance, both measuring power consumption and the two fundamental phases of execution. While the comparison presented here can help one choose among alternatives for the selected packages and algorithms, the problem of selecting a graph processing framework for a given large-scale problem remains far from simple. Hence, we make our code available at \url{https://github.com/HPCL/easy-parallel-graph} and encourage further experimentation. Our results required minor changes to the original projects to add calls to PAPI sampling and those changes are also available on forked versions of the repositories. 

Overall, the GAP Benchmark Suite was the highest-performing system across the given datasets though not the most scalable. However, this is only for relatively small graphs; all the graphs used here had at most $2^{22}$ vertices. It should be noted that GAP is also the most recent of projects. Thus, we recommend graph processing algorithm designers compare their BFS implementations to the Graph500 for BFS, GAP Benchmark Suite for smaller problem sizes or a low number of threads (at most 32), and GraphMat for larger problems with more threads. This is despite the fact that GraphBIG has the best scalability, because even at more than 16 threads the runtime for GraphBIG is still at least one order of magnitude slower than GAP or GraphMat for all sampled algorithms. One last consideration is price: GraphMat requires the Intel compiler collection.

%We found that GraphMat is only decently performant, but scales to more threads. One explanation is that GraphMat's underlying computation model (sparse matrix operations), paired with the increased overhead of GraphMat's doubly-compressed sparse row (DSCR) graph representation, is better suited to larger-scale graphs.
% Furthermore, sparse matrix operations have been well studied so achieving good scalability is more realistic [cite]

To ensure fairness, each platform must be configured to use the same graphs and the same roots. In the case of GraphBIG, this required the file to be loaded in for each experiment. By far the most time consuming of the experiments was GraphBIG's serial file input of uncompressed ASCII text format for graphs. This limited the sizes of the experiments we could run. Graphalytics also had circumstances with the more computationally expensive algorithms  where certain experiments fail \cite{Iosup:2016:Graphalyticstech}, so determining whether an algorithm will finish given a particular machine, input size, runtime limit, and resources is an important unanswered question we plan to pursue further.

\bibliographystyle{IEEEtran}
\bibliography{../drp}
\end{document}

% EuroPar Reviews
%----------------------- REVIEW 1 ---------------------
%PAPER: 204
%TITLE: A Comparison of Parallel Graph Processing Benchmarks
%AUTHORS: Samuel Pollard and Boyana Norris
%
%Overall evaluation: 0 (borderline paper)
%
%----------- Overall evaluation -----------
%Summary:
%Due to the large amount of existing graph processing systems, comprehensive comparisons need considerable effort. Therefore, it is difficult for developers to select the best approach. This paper presents an approach and associated software to analyze the performance and scalability of parallel, open-source graph libraries.
%
%Strengths:
%1. The motivation is clear, as this paper want to provide an approach to comprehensively compare the graph processing systems.
%2. It is a well-written paper.
%
%Weakness:
%1. In the contribution, you mentioned “automation of the installation”, however, I don’t think it is well demonstrated in this paper.
%2. All the graph data are synthetic graphs generated via Kronecker gen. As far as I know, these synthetic graphs have a very different degree distribution compared to real graphs. Can you add some real graphs in addition to synthetic. Many graphs are available at SNAP (Stanford).
%3. Some figures are confusing. For example, the right figure in “Figure 3” is named as PageRank Iterations. The value on y-axis is the time per iteration or the total execution time? If the value is the time for all iterations, then what is the difference between the left figure and right figure in “Figure 3”?
%
%Recommendations and Questions:
%1. Graph algorithms on each platform are implemented in different manners. When you directly compare their performance, it should be accompanied with deeper explanations.
%2. For strong scaling studies, there is no deep-dive to explain the poor scaling behaviors in GAP.
%3. Why use scale 20 graph for scaling study and scale 22 graph for other study? I think demonstrating the scaling behavior, we need the relatively big graph input.
%4. Most execution time for graph processing is spent on the framework ("Exploring big graph computing — An empirical study from architectural perspective", published in Journal of Parallel and Distributed Computing, 2016), focusing and comparing the framework can be another contribution that this paper can make.
%5. Use graphs from SNAP, as in the following papers. Please cite those papers as well. (i) Michael LeBeane et al, Supercomputing (SC 15), Data Partitioning Strategies for Graph Workloads on Heterogeneous Clusters (ii) Shuang Song et al,  ICPP 2016, Proxy-Guided Load Balancing of Graph Processing Workloads on Heterogeneous Clusters.
%
%
%----------------------- REVIEW 2 ---------------------
%PAPER: 204
%TITLE: A Comparison of Parallel Graph Processing Benchmarks
%AUTHORS: Samuel Pollard and Boyana Norris
%
%Overall evaluation: 1 (weak accept)
%
%----------- Overall evaluation -----------
%The paper performs a benchmarking study on Graph processing frameworks. The authors ran a large group of benchmarks to compare several frameworks in terms of the runtime and energy consumption using three different algorithm.
%
%While the paper is useful for audience who are looking for a starting point benchmark to select their Graph Processing framework.
%
%
%The paper has a good flow of information, however adding the following sections could make help the general audience to understand the value of the study.
%
%--> A comparison between the 5 graph processing frame works
%--> Justification about the selection of graph size - real world examples of graphs with these sizes of nodes/vertices.
%--> A more fine grained  analysis of the results and reasoning behind observed performance.
%
%Overall,the benchamrking study is of value for the research and mainly developer communities by helping them for selecting their framework.
%
%
%----------------------- REVIEW 3 ---------------------
%PAPER: 204
%TITLE: A Comparison of Parallel Graph Processing Benchmarks
%AUTHORS: Samuel Pollard and Boyana Norris
%
%Overall evaluation: -2 (reject)
%
%----------- Overall evaluation -----------
%This paper presents a comparison of runtime, scalability and energy
%consumption of 5 graph frameworks for 3 different algorithms,
%distinguishing the data-structure generation and algorithm execution
%phases.  Their experiments show that the fastest runtime is obtained
%with GAP Benchmark Suite for all of the 3 algorithms.  The most
%scalable framework is GraphBIG. Every frameworks show poor
%scalability beyond 32 threads.  In term of power and energy
%consumption the GAP benchmark is the most energy efficient whereas
%GraphMat exhibits the lowest average CPU and DRAM power consumption.
%
%
%The paper felt a bit confusing. To begin with, I could not find a clear
%exposition of the requirements and the methodology used and what are its
%implications. A precise list and exposition of the requirements and changes
%done to the application would help in understanding this. This is very critical
%due to the paper positionning itself as a comparison of multiple benchmarks.
%
%As for the experimental part, it is as confusing as the exposition of the
%contribution. Notably, contrarily to what the authors stated, I did not felt
%that they actually compare themselves to the results of graphalytics, they
%show some numbers from graphalytics but they are scarcely commented and they
%do not compare precisely the results to their methodology, highlight the
%differences and improvements, etc. A second problem concerns the size of the
%benchmarks run. In the conclusion, the authors say that they could only compare
%graphs of size 22 because of GraphBIG, but for the sake of this study why not
%exclude GraphBIG altogether in this case in order to have more results ? A third
%problem concerns the graph types. I understood that the graph of scale 22 is the
%Kronecker graph, and using elimination my guess is that the graph of scale 20 is
%the Dota-League Graph, but nowhere is this clearly stated which adds to the
%general confusion. Overall, using two graph sizes and types is an important limitation
%for graph algorithms. The paper could clearly benefit
%from a more in depth study of multiple graph types with different
%topologies, looking at the behavior of the multiple implementations with
%different graph sizes, etc.
%
%
%Details:
%Section 4.2: The left plot describes the second row of Table 3 -> the right plot ...
%
%
%----------------------- REVIEW 4 ---------------------
%PAPER: 204
%TITLE: A Comparison of Parallel Graph Processing Benchmarks
%AUTHORS: Samuel Pollard and Boyana Norris
%
%Overall evaluation: -2 (reject)
%
%----------- Overall evaluation -----------
%In this work, the authors attempt to compare parallel graph processing benchmarks. However, in fact, the paper proposes a benchmark of its own and pursues to demonstrate that it is superior to other benchmarks in terms of usability and performance.
%While the importance of graph benchmarking is increasing given the large number of systems and algorithms that need performance comparison, the paper falls short in delivering a clear comparison between existing benchmarks, and enough novel contributions to propose a new benchmark. Besides the brief explanation regarding power consumption, which is not common in the other benchmarks, there's no originality in this work.
%
%I have the following comments and concerns:
%- there's no criteria for comparing the benchmarks. While there's a discussion about usability and a separate discussion about failures and performance, the claim that the paper provides a rigorous comparison between these suites is not made true in the paper itself.
%- the comparison, as well as the newly proposed benchmark, seem to also ignore the distributed aspects of some of the frameworks, which other benchmarks take into account.
%- the newly proposed benchmark also has design flaws: the algorithms are not diverse enough, neither are the graphs. The authors do not give any explanation as to why such choices have been made, and therefore the proposed benchmark seems much more limited than all the previous ones.
%- the paper claims the proposed solution does not have the complexity of Graphalytics, where users need to write Java wrappers. However, in their case, PAPI instrumentation is needed, which might be (a) difficult to do, (b) not feasible in a production environment for users with insufficient credentials. How do these challenges affect the usability of the proposed method?
