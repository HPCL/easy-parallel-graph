#!/bin/bash
# Runs mst experiments on Arya
# This should be run in the easy-parallel-graph/autoperf/mst directory

USAGE="usage: ./run [--power] generate_data|experiment|parse|all
	. run [--power] build <------ do this one first"
# source run build to get the environment variables to stick.

# Parse command line options
unset POWER_PROFILING
for arg in "$@"; do
	case $arg in
	--power)
		export POWER_PROFILING=1
		shift
	;;
	*)	# Default
		# Do nothing
	esac
done
cmd="$1"
if [ -z "$cmd" -o "$cmd" = "-h" -o "$cmd" = "--help" ]; then
	echo "$USAGE"
	exit 2
elif [ "$cmd" != generate_data -a "$cmd" != experiment -a "$cmd" != build -a "$cmd" != all -a "$cmd" != parse ]; then
	echo "Invalid argument $cmd"
	echo "$USAGE"
	exit 2
fi

# Initialization
# These should all be the same as what's available in autoperf.cfg.
# This is why some are exported and you must source run build
LIB_DIR="$(pwd)/lib"
module load tau # Also loads the PAPI environment variable
export TAU_MAKEFILE="$TAU_DIR/x86_64/lib/Makefile.tau-papi-pdt-openmp" # I don't think this is needed when sampling
export DDIR=/home/users/spollard/graphalytics/all-datasets/PBBSInput
PBBS_DIR="$LIB_DIR/pbbs-msf/minSpanningForest"
XPS_DIR="$LIB_DIR/xpscode/MSTSC"
GALOIS_DIR="$LIB_DIR/Galois-2.2.1"
OUTPUTDIR="$(pwd)/output"
ENSURE_CONNECTED='no' # yes if you want to run bfs.out

# Example settings for synthetic datasets
#DATASETS=( com-friendster.ungraph com-orkut.ungraph com-lj.ungraph soc-pokec-relationships wiki-Talk protein.links.v10 )
# ^ Those are all the ones whose URLs are included in this file
#DATASETS=( com-lj.ungraph com-friendster.ungraph )
#REAL_VERTICES=( 3997962 65608366 )
#REAL_VERTICES=( 3998075 46565605 ) # This is from tr '[:blank:]' '\n' | sort -n | uniq | wc -l
#REAL_EDGES=( 34681189 1806067135 )
DATASETS=( com-lj.ungraph )
REAL_VERTICES=( 4036538  )
REAL_EDGES=( 34681189 )
# DATASETS=( facebook_combined ) # Test dataset, only 4,000 vertices
# REAL_VERTICES=( 4039 ) # Test
# REAL_EDGES=( 88234 ) # Test


# Example settings for synthetic datasets
# RT_TYPES="ER B G"
# INS_PCTAGES="25 50 75 100"
# CHANGED_VERTICES="5000 10000 15000"
# SCALES="20 21 22 23 24"
# RT_TYPES="ER B G" # Parameters are only known for these three

# Example settings for experiments
EPV=32 # Edges per vertex
THREADS="1 2 4 8 16 32 48 64 72"
NUM_BATCHES=10
NUM_TRIALS=8

# For batches:
THREADS="64"
CHANGED_VERTICES="10000"
INS_PCTAGES="75"
SCALES="23 24 25"
RT_TYPES="ER B"

# These will get overwritten; useful for one-off scripts
export OMP_NUM_THREADS=64 # If any generator is multithreaded
RT=ER
INS_PCT=75
CVERTS=10000
S=23

# Define commands to be run
generate_data()
{
	# Download or generate Mew MST graph files
	# Right now we just generate them on the fly
	# wget -nc -P "$DDIR" http://brix.d.cs.uoregon.edu/graphs/rmat{20..27}8_{B,G,ER}.gr.gz
	# echo "Unzipping files..."
	# for RT in $RT_TYPES; do
	# 	for S in $SCALES; do
	# 		gunzip -c "$DDIR/rmat${S}${EPV}_${RT}.gr.gz" | awk '{printf "%d %d %d\n",$1,$2,int(rand()*100)}' > "$DDIR/rmat${S}${EPV}_${RT}-orig.wel"
	# 		"$GALOIS_DIR/build/release/tools/graph-convert/graph-convert" -intedgelist2gr "$DDIR/rmat${S}${EPV}_${RT}-orig.wel" "$DDIR/rmat${S}${EPV}_${RT}-orig.wel"
	# 	done
	# done
	cd "$XPS_DIR"
	echo "Generating RMAT files..."
	for RT in $RT_TYPES; do
		for S in $SCALES; do
			for B in $(seq 0 $NUM_BATCHES); do
				PART_SUFFIX="${S}${EPV}_${RT}"
				if [ "$B" -eq 0 ]; then
					echo -e "~~~~~\nGenerating scale $S, $EPV edges per vertex, RMAT $RT\n~~~~~" # LOG
					if [ "$RT" = ER ]; then # Just do this once
						"$XPS_DIR/../RMAT/driverForRmat" $S 6 $EPV 0.45 0.15 0.15 0.25 "$DDIR/rmat${S}${EPV}_G-orig.wel"
						"$XPS_DIR/../RMAT/driverForRmat" $S 6 $EPV 0.55 0.15 0.15 0.15 "$DDIR/rmat${S}${EPV}_B-orig.wel"
						"$XPS_DIR/../RMAT/driverForRmat" $S 6 $EPV 0.25 0.25 0.25 0.25 "$DDIR/rmat${S}${EPV}_ER-orig.wel"
					fi
					mv "$DDIR/rmat${PART_SUFFIX}-orig.wel" tmp
					awk '{printf "%d %d %d\n", $1, $2, int(rand()*100)}' tmp > "$DDIR/rmat${PART_SUFFIX}-orig.wel"
					rm tmp
					"$XPS_DIR/tEx.out" "$DDIR/rmat${PART_SUFFIX}-orig.wel" $(awk '{print $1; exit}' "$DDIR/rmat${PART_SUFFIX}-orig.wel")
					# tEx.out generates Graph*x.txt, bfs.out generates Graph*.txt
					if [ "$ENSURE_CONNECTED" = 'yes' ]; then
						echo "WARNING: ENSURE_CONNECTED is not be supported anymore"
						"$XPS_DIR/bfs.out" "$XPS_DIR/GraphCx.txt" $(awk '{print $1; exit}' "$XPS_DIR/GraphCx.txt")
						rm "$XPS_DIR/Graphallx.txt" # May not be connected
						mv "$XPS_DIR/Graphall.txt" "$DDIR/rmat${PART_SUFFIX}.wel"
						rm "$XPS_DIR/GraphCx.txt" # Duplicate
						mv "$XPS_DIR/GraphC.txt" "$DDIR/rmat${PART_SUFFIX}.cert"
						rm "$XPS_DIR/Graphdiff.txt" # Empty
						mv "$XPS_DIR/Graphdiffx.txt" "$DDIR/rmat${PART_SUFFIX}.diff"
					else	
						mv "$XPS_DIR/Graphall.txt" "$DDIR/rmat${PART_SUFFIX}.wel"
						mv "$XPS_DIR/GraphC.txt" "$DDIR/rmat${PART_SUFFIX}.cert"
						mv "$XPS_DIR/Graphdiff.txt" "$DDIR/rmat${PART_SUFFIX}.diff"
					fi
					# No batches
					for INS_PCT in $INS_PCTAGES; do
						for CVERTS in $CHANGED_VERTICES; do	
							FULL_SUFFIX="${S}${EPV}_${RT}_${INS_PCT}i_${CVERTS}"
							"$XPS_DIR/cE.out" "$DDIR/rmat${PART_SUFFIX}.wel" $CVERTS 100 $INS_PCT > "$DDIR/changedrmat${FULL_SUFFIX}"
							sort -n -k1 -k2 "$DDIR/changedrmat${FULL_SUFFIX}" > "$DDIR/changedrmat${FULL_SUFFIX}S"
							awk -f "$LIB_DIR/../change_edgelist.awk" -v CEL="$DDIR/changedrmat${FULL_SUFFIX}S" "$DDIR/rmat${PART_SUFFIX}.wel" > "$DDIR/rmat${FULL_SUFFIX}.wel"
							"$GALOIS_DIR/build/release/tools/graph-convert/graph-convert" -intedgelist2gr "$DDIR/rmat${FULL_SUFFIX}.wel" "$DDIR/rmat${FULL_SUFFIX}.gr"
						done
					done
				else
					# Batches
					for INS_PCT in $INS_PCTAGES; do
						for CVERTS in $CHANGED_VERTICES; do	
							BATCH_SIZE=$(( $CVERTS / $NUM_BATCHES ))
							TOTAL_CHANGED=$(( $B * $BATCH_SIZE ))
							echo -e "~~~\nBatch $B, $TOTAL_CHANGED of $CVERTS changed vertices, $INS_PCT% insertions\n~~~" # LOG
							PREV_BATCH_SUFFIX="${S}${EPV}_${RT}_${INS_PCT}i_${CVERTS}_$(( $B - 1 ))b"
							FULL_SUFFIX="${S}${EPV}_${RT}_${INS_PCT}i_${CVERTS}_${B}b"
							if [ "$B" -eq 1 ]; then
								"$XPS_DIR/cE.out" "$DDIR/rmat${PART_SUFFIX}.wel" $BATCH_SIZE 100 $INS_PCT > "$DDIR/changedrmat${FULL_SUFFIX}"
								sort -n -k1 -k2 "$DDIR/changedrmat${FULL_SUFFIX}" > "$DDIR/changedrmat${FULL_SUFFIX}S"
								awk -f "$LIB_DIR/../change_edgelist.awk" -v CEL="$DDIR/changedrmat${FULL_SUFFIX}S" "$DDIR/rmat${PART_SUFFIX}.wel" > "$DDIR/rmat${FULL_SUFFIX}.wel"
								"$GALOIS_DIR/build/release/tools/graph-convert/graph-convert" -intedgelist2gr "$DDIR/rmat${FULL_SUFFIX}.wel" "$DDIR/rmat${FULL_SUFFIX}.gr"
							else
								"$XPS_DIR/tEx.out" "$DDIR/rmat${PREV_BATCH_SUFFIX}.wel" 1
								mv "$XPS_DIR/Graphall.txt" "$DDIR/rmat${FULL_SUFFIX}.wel"
								mv "$XPS_DIR/GraphC.txt" "$DDIR/rmat${FULL_SUFFIX}.cert"
								mv "$XPS_DIR/Graphdiff.txt" "$DDIR/rmat${FULL_SUFFIX}.diff"
								"$XPS_DIR/cE.out" "$DDIR/rmat${PREV_BATCH_SUFFIX}.wel" $BATCH_SIZE 100 $INS_PCT > "$DDIR/changedrmat${FULL_SUFFIX}"
								sort -n -k1 -k2 "$DDIR/changedrmat${FULL_SUFFIX}" > "$DDIR/changedrmat${FULL_SUFFIX}S"
								awk -f "$LIB_DIR/../change_edgelist.awk" -v CEL="$DDIR/changedrmat${FULL_SUFFIX}S" "$DDIR/rmat${PREV_BATCH_SUFFIX}.wel" > "$DDIR/rmat${FULL_SUFFIX}.wel"
								"$GALOIS_DIR/build/release/tools/graph-convert/graph-convert" -intedgelist2gr "$DDIR/rmat${FULL_SUFFIX}.wel" "$DDIR/rmat${FULL_SUFFIX}.gr"
							fi
						done
					done
				fi
			done
		done
	done
	cd "$LIB_DIR/.."
}

build_autoperf()
{
	if [ -z "$SQLITE3" ]; then
		echo "You may need SQLITE3 environment variable for autoperf. Trying to get it with dpkg..."
		export SQLITE3=$(dirname $(dpkg -L libsqlite3-dev | grep .so)) # Needed for autoperf
		if [ -z "$SQLITE3" ]; then
			echo "Unable to find libsqlite3 shared object file"
			echo "Try installing libsqlite3-dev or -devel"
			return 1
		else
			echo "SQLITE3 found at $SQLITE3"
		fi
	fi
	if [ -z "$PAPI" ]; then
		echo "Please set the PAPI environment variable"
		return 1
	fi
	# Download autoperf and add it to your PATH
	AUTOPERF_LOC=$(command -v autoperf) # Check if autoperf is already in path
	if [ $? -ne 0 ]; then
		echo "Installing autoperf"
		git clone https://github.com/HPCL/autoperf.git "$HOME/autoperf"
		cd "$HOME/autoperf"
		python setup.py install --user
	else
		echo "found autoperf at $AUTOPERF_LOC"
	fi
}

build_Galois()
{
	# XXX: This probably won't work for anyone else.
	#
	# This must be added to the CMakeLists.txt:
	# if(POWER_PROFILING)
	#   message(STATUS "Compiling for power measurement using RAPL")
	#   set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -DPOWER_PROFILING=1 -L${PAPI}/lib -Wl,-rpath,${PAPI}/lib")
	#   set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -DPOWER_PROFILING=1 -L${PAPI}/lib -Wl,-rpath,${PAPI}/lib")
	#   include_directories("${PAPI}/include")
	# endif()
	#
	# Replace apps/boruvka/CMakeLists.txt with the following:
	# app(boruvka Boruvka.cpp power_rapl.c power_rapl.h
	#   EXTLIBS papi)
	# app(boruvka-merge BoruvkaMerge.cpp)
	
	# Replace
	# Assumes Galois is already downloaded and unzipped into GALOIS_DIR
	# This can be found at
	# http://iss.ices.utexas.edu/projects/galois/downloads/Galois-2.2.1.tar.gz
	# The newer the version of boost, the newer you must have of cmake
    # module load boost-1.63.0-gcc-4.8-hucoocn
	module load boost/boost_1_53_0_gcc-4.8
	# Copy from easy-parallel-graph's RAPL wrappers.
    cd "$GALOIS_DIR/build"
	if [ "$POWER_PROFILING" = "1" ]; then
		mkdir -p power
		cd power
		cp "$LIB_DIR/../../../power/power_rapl.c" "$GALOIS_DIR/apps/boruvka"
		cp "$LIB_DIR/../../../power/power_rapl.h" "$GALOIS_DIR/apps/boruvka"
		cmake -DCMAKE_Fortran_COMPILER=gfortran-4.8 -DPOWER_PROFILING=1 -DPAPI="$PAPI" -Wno-dev ../../ 
	else
		mkdir -p release
		cd release
		cmake -DCMAKE_Fortran_COMPILER=gfortran-4.8 -Wno-dev ../../ 
	fi
}

build_PBBS()
{
	echo "building PBBS..."
	mkdir -p "$PBBS_DIR"
	wget -nc -P "$PBBS_DIR/.." http://www.cs.cmu.edu/~pbbs/benchmarks/minSpanningForest.tar 
	if [ ! -d minSpanningForest ]; then
		tar -xf minSpanningForest.tar
	fi
	cd minSpanningForest/parallelKruskal
	make
	cd "$LIB_DIR/.."
}

# Build XPS. This also sort of generates the data (though derived from the previous datasets)
build_XPS()
{
	git clone https://github.com/HPCL/xpscode.git "$LIB_DIR/xpscode" # Requires authentication
	cd "$XPS_DIR"
	ESSENSPATH="$LIB_DIR/xpscode/ESSENS"
	# Use vim syntax to edit the Makefile so it has the correct path to ESSENS
	ex -s Makefile "+:%s?^ESSENS=.*?ESSENS=$ESSENSPATH?g" '+:wq' # XXX: Will the * cause problems?
	# Turn on power monitoring (CFLAGS is used in c++ code too in this makefile)
	if [ "$POWER_PROFILING" = "1" ]; then
		CC=gcc
		RAPL_DIR="$LIB_DIR/../../../power"
		POWER_CFLAGS="-w -I$PAPI/include -I$RAPL_DIR -DPOWER_PROFILING=1"
		RAPL_INC="power_rapl.o $RAPL_DIR/power_rapl.h -L$PAPI/lib -Wl,-rpath,$PAPI/lib -lpapi"
		$CC -c $POWER_CFLAGS "$RAPL_DIR/power_rapl.c" -o power_rapl.o
		make CFLAGS="$POWER_CFLAGS" RAPL_INC="$RAPL_INC" all # This only builds the mst update executable (a.out)
	else
		make all
	fi
	make cE
	make tEx
	make bfs
}

run_autoperf()
{
	# FOR EXAMPLE...
	# TODO: Loop over these in a reasonable way, make AUTOPERF_NUM_THREADS actually done in autoperf.
	echo "CAUTION: Incomplete."
	AUTOPERF_SCALE=20
	AUTOPERF_CHANGED_EDGES=500
	AUTOPERF_MAX_WEIGHT=100
	AUTOPERF_NUM_VERTICES=$(echo '2 ^ ' $AUTOPERF_SCALE | bc)
	AUTOPERF_EDGES_PER_VERTEX=8 # XXX: Keep this at 8 for now
	AUTOPERF_NUM_THREADS=4

	# NOTE: When using ex you cannot also have that program open in vim or else it will hang
	cp autoperf.cfg .autoperf.cfg # Autoperf searches for .autoperf.cfg first
	ex -s .autoperf.cfg "+:%s/AUTOPERF_SCALE/$AUTOPERF_SCALE/g" '+:wq'
	ex -s .autoperf.cfg "+:%s/AUTOPERF_CHANGED_EDGES/$AUTOPERF_CHANGED_EDGES/g" '+:wq'
	ex -s .autoperf.cfg "+:%s/AUTOPERF_MAX_WEIGHT/$AUTOPERF_MAX_WEIGHT/g" '+:wq'
	ex -s .autoperf.cfg "+:%s/AUTOPERF_NUM_VERTICES/$AUTOPERF_NUM_VERTICES/g" '+:wq'
	ex -s .autoperf.cfg "+:%s/AUTOPERF_EDGES_PER_VERTEX/$AUTOPERF_EDGES_PER_VERTEX/g" '+:wq'
	ex -s .autoperf.cfg "+:%s/AUTOPERF_NUM_THREADS/$AUTOPERF_NUM_THREADS/g" '+:wq'
	# autoperf
}

generate_real()
{
	echo "Downloading datasets. May take a while..." # LOG
	wget -nc -P "$DDIR" https://snap.stanford.edu/data/bigdata/communities/com-friendster.ungraph.txt.gz
	wget -nc -P "$DDIR" https://snap.stanford.edu/data/bigdata/communities/com-orkut.ungraph.txt.gz
	wget -nc -P "$DDIR" https://snap.stanford.edu/data/bigdata/communities/com-lj.ungraph.txt.gz
	wget -nc -P "$DDIR" https://snap.stanford.edu/data/soc-pokec-relationships.txt.gz
	wget -nc -P "$DDIR" https://snap.stanford.edu/data/wiki-Talk.txt.gz
	wget -nc -P "$DDIR" http://string-db.org/download/protein.links.v10.txt.gz

	echo "Transforming real world datasets."
	echo "Unzipping the datasets. This only gets done once."
	for idx in $(seq 0 $(( ${#DATASETS[@]} - 1 )) ); do
		FILE_PREFIX="$DDIR/${DATASETS[$idx]}"
		if [ ! -f "$FILE_PREFIX.txt" ]; then
			echo "Unzipping $FILE_PREFIX.txt.gz..."
			gunzip -c "$FILE_PREFIX.txt.gz" > "$FILE_PREFIX.txt"
		else
			echo "File $FILE_PREFIX.txt already unzipped"
		fi
	done

	for idx in $(seq 0 $(( ${#DATASETS[@]} - 1 )) ); do
		echo "Transforming ${DATASETS[$idx]} into the correct formats"
		FILE_PREFIX="$DDIR/${DATASETS[$idx]}"
		awk '!/^#/{printf "%d %d %d\n", $1, $2, int(rand()*100)}' "$FILE_PREFIX.txt" > "$FILE_PREFIX.wel"
		"$XPS_DIR/tEx.out" "$FILE_PREFIX.wel" $(awk '{print $1; exit}' "$FILE_PREFIX.wel" )
		mv "Graphall.txt" "$FILE_PREFIX.wel"
		mv "GraphC.txt" "$FILE_PREFIX.cert"
		mv "Graphdiff.txt" "$FILE_PREFIX.diff"
		# No batches
		for INS_PCT in $INS_PCTAGES; do
			for CVERTS in $CHANGED_VERTICES; do	
				FILE_SUFFIX="_${INS_PCT}i_${CVERTS}"
				echo "Running on ${FILE_PREFIX}$FILE_SUFFIX (no batches)"
				"$XPS_DIR/cE.out" "$FILE_PREFIX.wel" $CVERTS 100 $INS_PCT > "${FILE_PREFIX}_changed${FILE_SUFFIX}"
				sort -n -k1 -k2 "${FILE_PREFIX}_changed${FILE_SUFFIX}" > "${FILE_PREFIX}_changed${FILE_SUFFIX}S"
				awk -f "$LIB_DIR/../change_edgelist.awk" -v CEL="${FILE_PREFIX}_changed${FILE_SUFFIX}S" "$FILE_PREFIX.wel" > "${FILE_PREFIX}$FILE_SUFFIX.wel"
				"$GALOIS_DIR/build/release/tools/graph-convert/graph-convert" -intedgelist2gr "${FILE_PREFIX}$FILE_SUFFIX.wel"  "${FILE_PREFIX}$FILE_SUFFIX.gr"
			done
		done
		# Batches
		BATCH_SIZE=$(( $CVERTS / $NUM_BATCHES ))
		for B in $(seq 1 $NUM_BATCHES); do
			for INS_PCT in $INS_PCTAGES; do
				for CVERTS in $CHANGED_VERTICES; do	
					TOTAL_CHANGED=$(( $B * $BATCH_SIZE ))
					PREV_BATCH_SUFFIX="_${INS_PCT}i_${CVERTS}_$(( $B - 1 ))b"
					FILE_SUFFIX="_${INS_PCT}i_${CVERTS}_${B}b"
					echo -e "~~~\nBatch $B, $TOTAL_CHANGED of $CVERTS changed vertices, $INS_PCT% insertions\n~~~" # LOG
					if [ "$B" -eq 1 ]; then
						"$XPS_DIR/cE.out" "$FILE_PREFIX.wel" $BATCH_SIZE 100 $INS_PCT > "${FILE_PREFIX}_changed${FILE_SUFFIX}"
						sort -n -k1 -k2 "${FILE_PREFIX}_changed${FILE_SUFFIX}" > "${FILE_PREFIX}_changed${FILE_SUFFIX}S"
						awk -f "$LIB_DIR/../change_edgelist.awk" -v CEL="${FILE_PREFIX}_changed${FILE_SUFFIX}S" "$FILE_PREFIX.wel" > "${FILE_PREFIX}$FILE_SUFFIX.wel"
						"$GALOIS_DIR/build/release/tools/graph-convert/graph-convert" -intedgelist2gr "${FILE_PREFIX}$FILE_SUFFIX.wel"  "${FILE_PREFIX}$FILE_SUFFIX.gr"
					else
						"$XPS_DIR/tEx.out" "${FILE_PREFIX}${PREV_BATCH_SUFFIX}.wel" $(awk '{if($1==0 || $1==1){print $1}else{print 1} exit}' "${FILE_PREFIX}${PREV_BATCH_SUFFIX}.wel")
						mv "Graphall.txt" "${FILE_PREFIX}${FILE_SUFFIX}.wel"
						mv "GraphC.txt" "${FILE_PREFIX}${FILE_SUFFIX}.cert"
						mv "Graphdiff.txt" "${FILE_PREFIX}${FILE_SUFFIX}.diff"
						"$XPS_DIR/cE.out" "${FILE_PREFIX}${PREV_BATCH_SUFFIX}.wel" $BATCH_SIZE 100 $INS_PCT > "${FILE_PREFIX}_changed${FILE_SUFFIX}"
						sort -n -k1 -k2 "${FILE_PREFIX}_changed${FILE_SUFFIX}" > "${FILE_PREFIX}_changed${FILE_SUFFIX}S"
						awk -f "$LIB_DIR/../change_edgelist.awk" -v CEL="${FILE_PREFIX}_changed${FILE_SUFFIX}S" "${FILE_PREFIX}${PREV_BATCH_SUFFIX}.wel" > "${FILE_PREFIX}${FILE_SUFFIX}.wel"
						"$GALOIS_DIR/build/release/tools/graph-convert/graph-convert" -intedgelist2gr "${FILE_PREFIX}${FILE_SUFFIX}.wel" "${FILE_PREFIX}${FILE_SUFFIX}.gr"
					fi
				done
			done
		done
	done
}

run_experiment()
{
	echo "Running the experiment..."
	mkdir -p "$OUTPUTDIR"
	cd "$OUTPUTDIR" # So we can just print output to cwd
	# TODO: more parameters and more loop levels
	# Adjust parameters
	SD='sudo'
	if [ "$POWER_PROFILING" = "1" ]; then
		GALOIS_MST="$GALOIS_DIR/build/power/apps/boruvka/boruvka"
	else
		GALOIS_MST="$GALOIS_DIR/build/release/apps/boruvka/boruvka"
		SD=''
	fi
	echo "Deleting previous log files..."
	for S in $SCALES; do
		for RT in $RT_TYPES; do
			for INS_PCT in $INS_PCTAGES; do
				for CVERTS in $CHANGED_VERTICES; do
					for OMP_NUM_THREADS in $THREADS; do
						rm -f "$OUTPUTDIR/{galois,mst}-${S}${EPV}_$RT_${INS_PCT}i_${CVERTS}_${OMP_NUM_THREADS}t.log"
					done
				done
			done
		done
	done
	for S in $SCALES; do
		for RT in $RT_TYPES; do
			for INS_PCT in $INS_PCTAGES; do
				for CVERTS in $CHANGED_VERTICES; do
					for OMP_NUM_THREADS in $THREADS; do
						for dummy in $(seq $NUM_TRIALS); do
							export OMP_NUM_THREADS
							N_VERT=$(echo 2 ^ $S | bc)
							FULL_SUFFIX="${S}${EPV}_${RT}_${INS_PCT}i_${CVERTS}"
							PART_SUFFIX="${S}${EPV}_${RT}"
							MSTLOG="$OUTPUTDIR/mst-${FULL_SUFFIX}_${OMP_NUM_THREADS}t.log"
							GALOISLOG="$OUTPUTDIR/galois-${FULL_SUFFIX}_${OMP_NUM_THREADS}t.log"
							BATCH_SIZE=$(( $CVERTS / $NUM_BATCHES ))
							echo -n "Running ${FULL_SUFFIX}_${OMP_NUM_THREADS}t, batch" # LOG
							if [ "$NUM_BATCHES" -gt 1 ]; then
								for B in $(seq $NUM_BATCHES); do
									echo -n " $B"
									echo "Galois Batch $B" >> "$GALOISLOG"
									$SD "$GALOIS_MST" -t=$OMP_NUM_THREADS "$DDIR/rmat${FULL_SUFFIX}_${B}b.gr" >> "$GALOISLOG"
								done
								echo #\n
							else
								$SD "$GALOIS_MST" -t=$OMP_NUM_THREADS "$DDIR/rmat${FULL_SUFFIX}.gr" >> "$GALOISLOG"
							fi
							# ./a.out <diff_file> <certificate> <set of changed edges> <upper bound of edge weight> <number of vertices> <batch size> <number of threads>
							$SD "$XPS_DIR/a.out" "$DDIR/rmat${PART_SUFFIX}.diff" "$DDIR/rmat${PART_SUFFIX}.cert" "$DDIR/changedrmat${FULL_SUFFIX}S" 100 $N_VERT $BATCH_SIZE $OMP_NUM_THREADS >> "$MSTLOG"
							$SD chown $USER "$MSTLOG"
							$SD chown $USER "$GALOISLOG"
						done
					done
				done
			done
		done
	done
	cd "$LIB_DIR/.."
}

run_real()
{
	echo "Running real world datasets."
	mkdir -p "$OUTPUTDIR"
	cd "$OUTPUTDIR" # So we can just print output to cwd
	# TODO: more parameters and more loop levels
	# Adjust parameters
	if [ "$POWER_PROFILING" = "1" ]; then
		GALOIS_MST="$GALOIS_DIR/build/power/apps/boruvka/boruvka"
		SD='sudo'
	else
		GALOIS_MST="$GALOIS_DIR/build/release/apps/boruvka/boruvka"
		SD=''
	fi
	echo "Deleting previous log files..."
	for idx in $(seq 0 $(( ${#DATASETS[@]} - 1 )) ); do
		DSET="${DATASETS[$idx]}"
		for INS_PCT in $INS_PCTAGES; do
			for CVERTS in $CHANGED_VERTICES; do
				for OMP_NUM_THREADS in $THREADS; do
					rm -f "$OUTPUTDIR/{galois,mst}-${DSET}_${INS_PCT}i_${CVERTS}_${OMP_NUM_THREADS}t.log"
				done
			done
		done
	done
	for idx in $(seq 0 $(( ${#DATASETS[@]} - 1 )) ); do
		N_VERT=${REAL_VERTICES[$idx]}
		DSET="${DATASETS[$idx]}"
		for INS_PCT in $INS_PCTAGES; do
			for CVERTS in $CHANGED_VERTICES; do
				for OMP_NUM_THREADS in $THREADS; do
					export OMP_NUM_THREADS
					for dummy in $(seq $NUM_TRIALS); do
						FILE_SUFFIX="_${INS_PCT}i_${CVERTS}"
						MSTLOG="mst-${DSET}${FILE_SUFFIX}_${OMP_NUM_THREADS}t.log"
						GALOISLOG="galois-${DSET}${FILE_SUFFIX}_${OMP_NUM_THREADS}t.log"
						BATCH_SIZE=$(( $CVERTS / $NUM_BATCHES ))
						echo -n "Running $DSET${FILE_SUFFIX}_${OMP_NUM_THREADS}t, batch" # LOG
						if [ "$NUM_BATCHES" -gt 1 ]; then
							for B in $(seq $NUM_BATCHES); do
								echo -n " $B"
								echo "Galois Batch $B" >> "$GALOISLOG"
								$SD "$GALOIS_MST" -t=$OMP_NUM_THREADS "$DDIR/${DSET}${FILE_SUFFIX}_${B}b.gr" >> "$GALOISLOG"
							done
							echo #\n
						else
							$SD "$GALOIS_MST" -t=$OMP_NUM_THREADS "$DDIR/${DSET}${FILE_SUFFIX}.gr" >> "$GALOISLOG"
						fi
						# ./a.out <diff_file> <certificate> <set of changed edges> <upper bound of edge weight> <number of vertices> <batch size> <number of threads>
						$SD "$XPS_DIR/a.out" "$DDIR/${DSET}.diff" "$DDIR/${DSET}.cert" "$DDIR/${DSET}_changed${FILE_SUFFIX}S" 100 $N_VERT $BATCH_SIZE $OMP_NUM_THREADS >> "$MSTLOG"
						$SD chown $USER "$MSTLOG"
						$SD chown $USER "$GALOISLOG"
					done
				done
			done
		done
	done
	cd "$LIB_DIR/.."
}

# parse_output helper function: parse one output file
# Expects the following variables to be set:
# PKG DATA_FORMAT GALOISLOG MSTLOG PFN NUM_BATCHES
parse_one()
{
	if [ "$POWER_PROFILING" = "1" ]; then
		# Galois
		awk -v PKG=$PKG -v INFO="$DATA_FORMAT" '/Total Energy.*PACKAGE_ENERGY:PACKAGE[0-9]+ \*/{c++;if(c%PKG==0){printf "Galois,All,%s,Total CPU Energy (J),%s\n",INFO,t+$3;t=0}else{t+=$3}}' "${GALOISLOG}" >> "$PFN"
		awk -v PKG=$PKG -v INFO="$DATA_FORMAT" '/Average.*PACKAGE_ENERGY:PACKAGE[0-9]+ \*/{c++;if(c%PKG==0){printf "Galois,All,%s,Average CPU Power (W),%s\n",INFO,t+$3;t=0}else{t+=$3}}' "${GALOISLOG}" >> "$PFN"
		awk -v PKG=$PKG -v INFO="$DATA_FORMAT" '/^[0-9]+\.[0-9]+ s/{printf "Galois,All,%s,RAPL Time (s),%s\n",INFO,$1;exit}' ${GALOISLOG} >> "$PFN"
		# MST
		awk -v PKG=$PKG -v INFO="$DATA_FORMAT" '/Total Energy.*PACKAGE_ENERGY:PACKAGE[0-9]+ \*/{c++;if(c%PKG==0){printf "MST,%s,All,Total CPU Energy (J),%s\n",INFO,t+$3;t=0}else{t+=$3}}' "${MSTLOG}" >> "$PFN"
		awk -v PKG=$PKG -v INFO="$DATA_FORMAT" '/Average.*PACKAGE_ENERGY:PACKAGE[0-9]+ \*/{c++;if(c%PKG==0){printf "MST,%s,All,Average CPU Power (W),%s\n",INFO,t+$3;t=0}else{t+=$3}}' "${MSTLOG}" >> "$PFN"
		awk -v PKG=$PKG -v INFO="$DATA_FORMAT" '/^[0-9]+\.[0-9]+ s/{printf "MST,%s,All,RAPL Time (s),%s\n",INFO,$1;exit}' "${MSTLOG}" >> "$PFN"
	fi
	if [ "$NUM_BATCHES" -le 1 ]; then 
		# Galois
		awk -F ',' -v INFO="$DATA_FORMAT" '/,Time,/{printf "Galois,All,%s,Time (s),%s\n",INFO,$5 / 1000}' "${GALOISLOG}" >> "$PFN"
		# MST
		awk -v INFO="$DATA_FORMAT" '/Total Time for Creating Initial Rooted Tree at iter/{printf "MST,rooting tree,%s,Time (s),%s\n",INFO,$12}' "${MSTLOG}" >> "$PFN"
		awk -v INFO="$DATA_FORMAT" '/Time for First Pass/{printf "MST,first pass,%s,Time (s),%s\n",INFO,$5}' "${MSTLOG}" >> "$PFN"
		awk -v INFO="$DATA_FORMAT" '/Time for Classify and insert/{printf "MST,insertion,%s,Time (s),%s\n",INFO,$6}' "${MSTLOG}" >> "$PFN"
		awk -v INFO="$DATA_FORMAT" '/Time for Deletion/{printf "MST,deletion,%s,Time (s),%s\n",INFO,$4}' "${MSTLOG}" >> "$PFN"
		awk -v INFO="$DATA_FORMAT" '/Total Time for Updating/{if (NF == 5){printf "MST,All,%s,Time (s),%s\n",INFO,$5}}' "${MSTLOG}" >> "$PFN"
	else
		for B in $(seq $NUM_BATCHES); do
			INFO="${DATA_FORMAT},$B"
			# Galois
			grep -A 23 "Galois Batch $B" "${GALOISLOG}" |  awk -F ',' -v INFO="$INFO" '/,Time,/{printf "Galois,All,%s,Time (s),%s\n",INFO,$5 / 1000}' "${GALOISLOG}" >> "$PFN"
			# MST
			if [ "$B" -eq 1 ]; then
				awk -v INFO="$INFO" '/Total Time for Creating Initial Rooted Tree at iter 1/{printf "MST,rooting tree,%s,Time (s),%s\n",INFO,$12}' "${MSTLOG}" >> "$PFN"
			else
				THIS_BATCH=$(grep -B 2 -A 8 "Total Time for Updating Rooted Tree at iter $B" "$MSTLOG")
				echo "$THIS_BATCH" | awk -v INFO="$INFO" '/Total Time for Updating Rooted Tree at iter/{printf "MST,updating tree,%s,Time (s),%s\n",INFO,$11}' "${MSTLOG}" >> "$PFN"

				echo "$THIS_BATCH" | awk -v INFO="$INFO" '/Time for Classify and insert/{printf "MST,insertion,%s,Time (s),%s\n",INFO,$6}' "${MSTLOG}" >> "$PFN"
				echo "$THIS_BATCH" | awk -v INFO="$INFO" '/Time for Deletion/{printf "MST,deletion,%s,Time (s),%s\n",INFO,$4}' "${MSTLOG}" >> "$PFN"
			fi
		done
	fi
}

parse_output()
{
	echo "Parsing output..."
	if [ "$POWER_PROFILING" = "1" ]; then
		PFN="$OUTPUTDIR/parsed-power-aggregate.txt"
	else
		PFN="$OUTPUTDIR/parsed-aggregate.txt"
	fi
	PKG=2
	# Print the header
	if [ "$NUM_BATCHES" -le 1 ]; then 
		echo 'algorithm,execution_phase,scale,edges_per_vertex,RMAT_type,insertion_percent,changed_vertices,threads,measurement,value' > "$PFN"
	else
		echo 'algorithm,execution_phase,scale,edges_per_vertex,RMAT_type,insertion_percent,changed_vertices,threads,batch,measurement,value' > "$PFN"
	fi
	for RT in $RT_TYPES; do
		for S in $SCALES; do
			for INS_PCT in $INS_PCTAGES; do
				for CVERTS in $CHANGED_VERTICES; do	
					for T in $THREADS; do
						for S in $SCALES; do
							MSTLOG="$OUTPUTDIR/mst-${S}${EPV}_${RT}_${INS_PCT}i_${CVERTS}_${T}t.log"
							GALOISLOG="$OUTPUTDIR/galois-${S}${EPV}_${RT}_${INS_PCT}i_${CVERTS}_${T}t.log"
							DATA_FORMAT="$S,$EPV,$RT,$INS_PCT,$CVERTS,$T"
							parse_one
						done
					done
				done
			done
		done
	done
}
# Old parse output
# The extra steps for grep are unnecessary since we save each run in a different file.
			# grep -A 30 'Starting create_tree' ${MSTLOG} | awk -v PKG=$PKG -v S=$S -v EPV=$EPV -v RMT=$RT '/Average.*PACKAGE_ENERGY:PACKAGE[0-9]+ \*/{c++;if(c%PKG==0){printf "MST,create_tree,%s_%s_%s,Average CPU Power (W),%f\n",S,EPV,RMT,t+$3;t=0}else{t+=$3}}' >> "$PFN"
			# grep -A 30 'Starting create_tree' ${MSTLOG} | awk -v PKG=$PKG -v S=$S -v EPV=$EPV -v RMT=$RT '/^[0-9]+\.[0-9]+ s/{printf "MST,create_tree,%s_%s_%s,Time (s),%s\n",S,EPV,RMT,$1;exit}' >> "$PFN"

			# grep -A 30 'Starting first_pass' ${MSTLOG} | awk -v PKG=$PKG -v S=$S -v EPV=$EPV -v RMT=$RT '/Total Energy.*PACKAGE_ENERGY:PACKAGE[0-9]+ \*/{c++;if(c%PKG==0){printf "MST,first_pass,%s_%s_%s,Total CPU Energy (J),%f\n",S,EPV,RMT,t+$3;t=0}else{t+=$3}}' >> "$PFN"
			# grep -A 30 'Starting first_pass' ${MSTLOG} | awk -v PKG=$PKG -v S=$S -v EPV=$EPV -v RMT=$RT '/Average.*PACKAGE_ENERGY:PACKAGE[0-9]+ \*/{c++;if(c%PKG==0){printf "MST,first_pass,%s_%s_%s,Average CPU Power (W),%f\n",S,EPV,RMT,t+$3;t=0}else{t+=$3}}' >> "$PFN"
			# grep -A 30 'Starting first_pass' ${MSTLOG} | awk -v PKG=$PKG -v S=$S -v EPV=$EPV -v RMT=$RT '/^[0-9]+\.[0-9]+ s/{printf "MST,first_pass,%s_%s_%s,Time (s),%s\n",S,EPV,RMT,$1;exit}' >> "$PFN"

			# grep -A 30 'Starting process_insertions' ${MSTLOG} | awk -v PKG=$PKG -v S=$S -v EPV=$EPV -v RMT=$RT '/Total Energy.*PACKAGE_ENERGY:PACKAGE[0-9]+ \*/{c++;if(c%PKG==0){printf "MST,process_insertions,%s_%s_%s,Total CPU Energy (J),%f\n",S,EPV,RMT,t+$3;t=0}else{t+=$3}}' >> "$PFN"
			# grep -A 30 'Starting process_insertions' ${MSTLOG} | awk -v PKG=$PKG -v S=$S -v EPV=$EPV -v RMT=$RT '/Average.*PACKAGE_ENERGY:PACKAGE[0-9]+ \*/{c++;if(c%PKG==0){printf "MST,process_insertions,%s_%s_%s,Average CPU Power (W),%f\n",S,EPV,RMT,t+$3;t=0}else{t+=$3}}' >> "$PFN"
			# grep -A 30 'Starting process_insertions' ${MSTLOG} | awk -v PKG=$PKG -v S=$S -v EPV=$EPV -v RMT=$RT '/^[0-9]+\.[0-9]+ s/{printf "MST,process_insertions,%s_%s_%s,Time (s),%s\n",S,EPV,RMT,$1;exit}' >> "$PFN"

			# grep -B 31 'Time for Deletion' ${MSTLOG} | awk -v PKG=$PKG -v S=$S -v EPV=$EPV -v RMT=$RT '/Total Energy.*PACKAGE_ENERGY:PACKAGE[0-9]+ \*/{c++;if(c%PKG==0){printf "MST,process_deletions,%s_%s_%s,Total CPU Energy (J),%f\n",S,EPV,RMT,t+$3;t=0}else{t+=$3}}' >> "$PFN"
			# grep -B 31 'Time for Deletion' ${MSTLOG} | awk -v PKG=$PKG -v S=$S -v EPV=$EPV -v RMT=$RT '/Average.*PACKAGE_ENERGY:PACKAGE[0-9]+ \*/{c++;if(c%PKG==0){printf "MST,process_deletions,%s_%s_%s,Average CPU Power (W),%f\n",S,EPV,RMT,t+$3;t=0}else{t+=$3}}' >> "$PFN"
			# grep -B 31 'Time for Deletion' ${MSTLOG} | awk -v PKG=$PKG -v S=$S -v EPV=$EPV -v RMT=$RT '/^[0-9]+\.[0-9]+ s/{printf "MST,process_deletions,%s_%s_%s,Time (s),%s\n",S,EPV,RMT,$1;exit}' >> "$PFN"

###
# "Main"
###
if [ "$cmd" = generate_data -o "$cmd" = all ]; then
	#generate_data
	generate_real
fi
if [ "$cmd" = build -o "$cmd" = all ]; then
	# build_autoperf
    build_Galois
	# build_PBBS
	build_XPS
	cd "$LIB_DIR/.."
fi

if [ "$cmd" = parse -o "$cmd" = all ]; then
	parse_output
fi

if [ "$cmd" = experiment -o "$cmd" = all ]; then
	# run_autoperf
	# run_experiment
	run_real
fi

