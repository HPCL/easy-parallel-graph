#!/bin/bash
# Runs mst experiments on Arya
# NOTE: You may need to execute
#       . run build
# To get the environment variables to stick.
# TODO: Add editing of autoperf.cfg to shell script? Or can we just export DDIR and stuff and be okay?

USAGE="usage: ./run generate_data|experiment|all
	. run build <------ do this one first"

# Parse command line options
cmd="$1"
if [ -z "$cmd" -o "$cmd" = "-h" -o "$cmd" = "--help" ]; then
	echo "$USAGE"
	return 2
fi

# Initialization
LIB_DIR="$(pwd)/lib"
module load tau # Also loads PAPI
export TAU_MAKEFILE="$TAU_DIR/x86_64/lib/Makefile.tau-papi-openmp"
export DDIR=/home/users/spollard/graphalytics/all-datasets/PBBSInput
PBBS_DIR="$LIB_DIR/pbbs-msf/minSpanningForest"
XPS_DIR="$LIB_DIR/xpscode/WorkingCode"
SCALES="16 18 20" # Make sure to have SCALES be the same as what's in autoperf.cfg

# Define commands to be run
generate_data()
{
	for S in $SCALES; do
		N_VERT=$(echo 2 ^ $S | bc)
		M_EDGE=$(echo $N_VERT '* 8' | bc) # Graph500 spec is 16, XPS code has 8 by default
		WEL_FILE="$DDIR/rmat$S.wel"
		if [ -r "$WEL_FILE" ]; then
			echo "files already exists at $DDIR/rmat$S.*, avoiding overwriting." # LOG
			continue
		fi
		# Make .wel (for XPS and conversion to .gr)
		# Usage: ../RMAT/driverForRmat <SCALE> <SC-WT> <a b c d> <Output filename>
		# Details: SCALE = #Vertices = 2^SCALE  -- #Edges = 8*#Vertices
		# SC-WT: Random weights in the range zero to 2^SC-WT will be assigned
		cd "$XPS_DIR"
		echo "create RMAT with 2^$S vertices and ~8 x 2^$S edges at $DDIR/xps-rmat${S}.wel" # LOG
		../RMAT/driverForRmat ${S} 6 0.25 0.25 0.25 0.25 "$DDIR/xps-rmat${S}.wel"

		# tEx.out requires the first line to begin with 0 or 1
		awk '{print $1 " " $2 " " int(rand()*100)}' "$DDIR/xps-rmat${S}.wel" > tmp
		sort -n -k1 -k2 tmp > "$DDIR/xps-rmat${S}.wel" # XXX We don't want to have to sort this.
		rm tmp

		# Create the MST Tree and the Remaining Graph
		# ./tEx <filename> <starts with 1 or 0>
		./tEx.out "$DDIR/xps-rmat${S}.wel" $(awk '{print $1; exit}' "$DDIR/xps-rmat${S}.wel") # XXX: 2nd arg may not be 0 or 1 (but seems to be so with a=b=c=d=0.25 rmat parameters)
		# This will create three files
		# Graphall.txt (Original file, starting from 0, undirected)
		# GraphC.txt  (the certificate files, MST/BFS)
		# Graphdiff.txt  (remaining edges, not in certificate)
		# Store the generated files with correct names
		mv Graphall.txt "$DDIR/rmat${S}.wel"
		mv GraphC.txt "$DDIR/rmat${S}.cert"
		mv Graphdiff.txt "$DDIR/rmat${S}.diff"

		# Create set of changed Edges  (create_edgelist.cpp)
		# ./cE.out <filename> <number of edges>
		./cE.out "$DDIR/rmat${S}.wel" 500 > "$DDIR/changedrmat${S}_500"

		# Sort the edges for faster run
		# sort -n -k1 <filename> > output
		sort -n -k1 -k2 "$DDIR/changedrmat${S}_500" > "$DDIR/changedrmat${S}_500S"
		cd "$LIB_DIR/.."

		# Make .weg (for PBBS)
		echo -e "WeightedEdgeArray\n$(cat $DDIR/rmat$S.wel)" > "$DDIR/rmat$S.weg"
		echo "saved to $DDIR/rmat$S.weg" # LOG

		# Make .gr file for Galois
		"$GALOIS_DIR/build/release/tools/graph-convert/graph-convert" -doubleedgelist2gr "$DDIR/rmat$S.wel" "$DDIR/rmat$S.gr"
	done
	echo "Downloading datasets. May take a while..." # LOG
	wget -nc -P "$DDIR" https://snap.stanford.edu/data/bigdata/communities/com-friendster.ungraph.txt.gz
	wget -nc -P "$DDIR" https://snap.stanford.edu/data/bigdata/communities/com-orkut.ungraph.txt.gz
	wget -nc -P "$DDIR" https://snap.stanford.edu/data/bigdata/communities/com-lj.ungraph.txt.gz
	wget -nc -P "$DDIR" https://snap.stanford.edu/data/soc-pokec-relationships.txt.gz
	wget -nc -P "$DDIR" https://snap.stanford.edu/data/wiki-Talk.txt.gz
	wget -nc -P "$DDIR" http://string-db.org/download/protein.links.v10.txt.gz
}

build_autoperf()
{
	if [ -z "$SQLITE3" ]; then
		echo "You may need SQLITE3 environment variable for autoperf. Trying to get it with dpkg."
		export SQLITE3=$(dirname $(dpkg -L libsqlite3-dev | grep .so)) # Needed for autoperf
		if [ -z "$SQLITE3" ]; then
			echo "Unable to find libsqlite3 shared object file"
			return 1
		fi
	fi
	if [ -z "$PAPI" ]; then
		echo "Please set the PAPI environment variable"
		return 1
	fi
	# Download autoperf and add it to your PATH
	AUTOPERF_LOC=$(command -v autoperf) # Check if autoperf is already in path
	if [ $? -ne 0 ]; then
		echo "Installing autoperf"
		git clone https://github.com/HPCL/autoperf.git "$HOME/autoperf"
		cd "$HOME/autoperf"
		python setup.py install --user
	else
		echo "found autoperf at $AUTOPERF_LOC"
	fi
}

build_Galois()
{
	GALOIS_DIR="$LIB_DIR/Galois-2.2.1"
    module load boost-1.63.0-gcc-4.8-hucoocn
    cd "$GALOIS_DIR/build"
    mkdir -p release
    cd release
    cmake -DCMAKE_Fortran_COMPILER=gfortran-4.8 ../../ 
}

build_PBBS()
{
	# TODO
	echo "Assumes PBBS is built in the right place."
}

# Build XPS. This also sort of generates the data (though derived from the previous datasets)
build_XPS()
{
	git clone https://github.com/HPCL/xpscode.git "$LIB_DIR/xpscode" # Requires authentication
	cd "$XPS_DIR"
	ESSENSPATH="$LIB_DIR/xpscode/ESSENS"
	# Use vim syntax to edit the Makefile so it has the correct path to ESSENS
	ex -s Makefile "+:%s?^ESSENS=.*?ESSENS=$ESSENSPATH?g" '+:wq' # XXX: Will the * cause problems?
	make all # This only builds the mst update executable (a.out)
	make cE
	make tEx
	make lpath
	make permF

	# To actually run the update (will be pushed to autoperf)
	# Run the update (main_code1.cpp)
	# ./a.out <diff_file> <certificate> <set of changed edges>  <number of vertices>  <number of threads>
	# ./a.out "$DDIR/XPS/rmat18.diff" "$DDIR/XPS/rmat18.changes" "$DDIR/XPS/changedrmat18_500S" 262144 4
}

run_autoperf()
{
	autoperf
}

run_experiment()
{
	echo "Running the experiment..."
}

# Execute
if [ "$cmd" = generate_data -o "$cmd" = all ]; then
	generate_data
fi
if [ "$cmd" = build -o "$cmd" = all ]; then
	build_autoperf
    build_Galois
	build_PBBS
	build_XPS
	cd "$LIB_DIR/.."
fi
if [ "$cmd" = experiment -o "$cmd" = all ]; then
	# run_autoperf
	run_experiment
fi

